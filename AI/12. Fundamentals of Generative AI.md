<h2>Introduction</h2>

AI – just the application of mathematical techniques that have been incrementally discovered and refined over many years of research into statistics, data science, and machine learning

# What is generative AI?


Artificial Intelligence (AI) imitates human behavior by using machine learning to interact with the environment and execute tasks without explicit directions on what to output.

# Large language models


Generative AI applications are powered by _large language models_ (LLMs), which are a specialized type of machine learning model that you can use to perform _natural language processing_ (NLP) tasks, including:

- Determining _sentiment_ or otherwise classifying natural language text.
- Summarizing text.
- Comparing multiple text sources for semantic similarity.
- Generating new natural language.

While the mathematical principles behind these LLMs can be complex, a basic understanding of the architecture used to implement them can help you gain a conceptual understanding of how they work.

## Transformer models

Machine learning models for natural language processing have evolved over many years. Today's cutting-edge large language models are based on the _transformer_ architecture, which builds on and extends some techniques that have been proven successful in modeling _vocabularies_ to support NLP tasks - and in particular in generating language. Transformer models are trained with large volumes of text, enabling them to represent the semantic relationships between words and use those relationships to determine probable sequences of text that make sense. Transformer models with a large enough vocabulary are capable of generating language responses that are tough to distinguish from human responses.

Transformer model architecture consists of two components, or _blocks_:

- An _encoder_ block that creates semantic representations of the training vocabulary.
- A _decoder_ block that generates new language sequences.
In practice, the specific implementations of the architecture vary – for example, the Bidirectional Encoder Representations from Transformers (BERT) model developed by Google to support their search engine uses only the encoder block, while the Generative Pretrained Transformer (GPT) model developed by OpenAI uses only the decoder block.

While a complete explanation of every aspect of transformer models is beyond the scope of this module, an explanation of some of the key elements in a transformer can help you get a sense for how they support generative AI.

### Tokenization

The first step in training a transformer model is to decompose the training text into _tokens_ - in other words, identify each unique text value

### Embeddings

While it may be convenient to represent tokens as simple IDs - essentially creating an index for all the words in the vocabulary, they don't tell us anything about the meaning of the words, or the relationships between them. To create a vocabulary that encapsulates semantic relationships between the tokens, we define contextual vectors, known as _embeddings_, for them. Vectors are multi-valued numeric representations of information, for example [10, 3, 1] in which each numeric element represents a particular attribute of the information. For language tokens, each element of a token's vector represents some semantic attribute of the token. The specific categories for the elements of the vectors in a language model are determined during training based on how commonly words are used together or in similar contexts.


The _encoder_ and _decoder_ blocks in a transformer model include multiple layers that form the neural network for the model

_attention_ layers. Attention is a technique used to examine a sequence of text tokens and try to quantify the strength of the relationships between them. In particular, _self-attention_ involves considering how other tokens around one particular token influence that token's meaning.

In an encoder block, each token is carefully examined in context, and an appropriate encoding is determined for its vector embedding. The vector values are based on the relationship between the token and other tokens with which it frequently appears. This contextualized approach means that the same word might have multiple embeddings depending on the context in which it's used - for example "the bark of a tree" means something different to "I heard a dog bark."

In a decoder block, attention layers are used to predict the next token in a sequence. For each token generated, the model has an attention layer that takes into account the sequence of tokens up to that point. The model considers which of the tokens are the most influential when considering what the next token should be. For example, given the sequence “I heard a dog,” the attention layer might assign greater weight to the tokens “heard” and “dog” when considering the next word

During training, the goal is to predict the vector for the final token in the sequence based on the preceding tokens. The attention layer assigns a numeric _weight_ to each token in the sequence so far. It uses that value to perform a calculation on the weighted vectors that produces an _attention score_ that can be used to calculate a possible vector for the next token. In practice, a technique called _multi-head attention_ uses different elements of the embeddings to calculate multiple attention scores. A neural network is then used to evaluate all possible tokens to determine the most probable token with which to continue the sequence. The process continues iteratively for each token in the sequence, with the output sequence so far being used regressively as the input for the next iteration – essentially building the output one token at a time.
![[Pasted image 20240416010416.png]]



1. A sequence of token embeddings is fed into the attention layer. Each token is represented as a vector of numeric values.
2. The goal in a decoder is to predict the next token in the sequence, which will also be a vector that aligns to an embedding in the model’s vocabulary.
3. The attention layer evaluates the sequence so far and assigns weights to each token to represent their relative influence on the next token.
4. The weights can be used to compute a new vector for the next token with an attention score. Multi-head attention uses different elements in the embeddings to calculate multiple alternative tokens.
5. A fully connected neural network uses the scores in the calculated vectors to predict the most probable token from the entire vocabulary.
6. The predicted output is appended to the sequence so far, which is used as the input for the next iteration.

During training, the actual sequence of tokens is known – we just mask the ones that come later in the sequence than the token position currently being considered. As in any neural network, the predicted value for the token vector is compared to the actual value of the next vector in the sequence, and the loss is calculated. The weights are then incrementally adjusted to reduce the loss and improve the model. When used for inferencing (predicting a new sequence of tokens), the trained attention layer applies weights that predict the most probable token in the model’s vocabulary that is semantically aligned to the sequence so far.


# What is Azure OpenAI?


Azure OpenAI Service is Microsoft's cloud solution for deploying, customizing, and hosting large language models. It brings together the best of OpenAI's cutting edge models and APIs with the security and scalability of the Azure cloud platform. Microsoft's partnership with OpenAI enables Azure OpenAI users to access the latest language model innovations.

Azure OpenAI supports many models that can serve different needs. These models include:

- **GPT-4 models** are the latest generation of _generative pretrained_ (GPT) models that can generate natural language and code completions based on natural language prompts.
- **GPT 3.5 models** can generate natural language and code completions based on natural language prompts. In particular, **GPT-35-turbo** models are optimized for chat-based interactions and work well in most generative AI scenarios.
- **Embeddings models** convert text into numeric vectors, and are useful in language analytics scenarios such as comparing text sources for similarities.
- **DALL-E models** are used to generate images based on natural language prompts. Currently, DALL-E models are in preview. DALL-E models aren't listed in the Azure OpenAI Studio interface and don't need to be explicitly deployed

# What are copilots?



The availability of LLMs has led to the emergence of a new category of computing known as copilots. Copilots are often integrated into other applications and provide a way for users to get help with common tasks from a generative AI model. Copilots are based on a common architecture, so developers can build custom copilots for various business-specific applications and services.


# Improve generative AI responses with prompt engineering

## System messages

Prompt engineering techniques include defining a system message. The message sets the context for the model by describing expectations and constraints, for example, "You're a helpful assistant that responds in a cheerful, friendly manner". These system messages determine constraints and styles for the model's responses.

## Writing good prompts

You can get the most useful completions by being explicit about the kind of response you want, for example, “Create a list of 10 things to do in Edinburgh during August”. You can achieve better results when you submit clear, specific prompts.

## Providing examples

LLMs generally support _zero-shot learning_ in which responses can be generated without prior examples. However, you can also provide _one-shot_ learning prompts that include one, or a few, examples of the output you require such as, “Visit the castle in the morning before the crowds arrive”. The model can then generate further responses in the same style as the examples provided in the prompt.

## Grounding data

Prompts can include _grounding_ data to provide context. You can use grounding data as a prompt engineering technique to gain many of the benefits of fine-tuning without having to train a custom model.


# Knowledge check


1.

What are Large Language Models?

Models that only work with one language.

Models that only work with small amounts of data.

Models that use deep learning to process and understand natural language on a massive scale.

Correct. Large language models use deep learning to process and understand natural language on a massive scale.

2.

What is an example of a potential task a generative AI application can help solve?

Monitoring the temperature in a manufacturing facility.

Creating a draft for an email.

Creating a draft for an email is a generative AI task.

Collecting real time data and storing it in a database.

3.

What is the purpose of vector-based embeddings?

To represent semantic meaning of text tokens.

Correct. The embedding for each token consists of multiple numeric elements. Each element indicates the location of the word along a particular dimension, like coordinates on a map.

To create tokens that include multiple representations of a word in different languages.

To correct misspellings in the training data.

4.

What is the potential impact of copilots?

Copilots only impact applications used in professional settings.

Copilots can help with first drafts, information synthesis, strategic planning, and much more.

Correct. Copilots have the potential to revolutionize the way we work

Copilots can only be used for certain natural language tasks like summarizing text.

# Summary


In this module, you have learned about generative AI, a branch of artificial intelligence that creates new content based on natural language input. Generative AI is typically built into software applications and uses language models trained with huge volumes of textual data to generate human-like natural language responses or even original images. You have also learned about the core concepts related to large language models and how generative AI is being integrated into Microsoft technologies with copilots.

The main takeaways from this module are that generative AI has the potential to revolutionize the way we work by assisting with first drafts, information synthesis, and strategic planning. Copilots, which are AI-powered assistants based on large language models, can be customized for various business-specific applications and services. The quality of responses that a generative AI application returns not only depends on the model itself but also on the types of prompts it's given. Prompt engineering is the process of improving prompts to produce better results.