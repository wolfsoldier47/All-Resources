## Convolutional neural networks (CNNs)

One of the most common machine learning model architectures for computer vision is a _convolutional neural network_ (CNN). CNNs use filters to extract numeric feature maps from images, and then feed the feature values into a deep learning model to generate a label prediction
![[Pasted image 20240414190601.png]]

1. Images with known labels (for example, 0: apple, 1: banana, or 2: orange) are fed into the network to train the model.
2. One or more layers of filters is used to extract features from each image as it is fed through the network. The filter kernels start with randomly assigned weights and generate arrays of numeric values called _feature maps_.
3. The feature maps are flattened into a single dimensional array of feature values.
4. The feature values are fed into a fully connected neural network.
5. The output layer of the neural network uses a _softmax_ or similar function to produce a result that contains a probability value for each possible class, for example [0.2, 0.5, 0.3].

### Transformers

Most advances in computer vision over the decades have been driven by improvements in CNN-based models. However, in another AI discipline - _natural language processing_ (NLP), another type of neural network architecture, called a _transformer_ has enabled the development of sophisticated models for language. Transformers work by processing huge volumes of data, and encoding language _tokens_ (representing individual words or phrases) as vector-based _embeddings_ (arrays of numeric values)

![[Pasted image 20240414190903.png]]
Tokens that are semantically similar are encoded in similar positions, creating a semantic language model that makes it possible to build sophisticated NLP solutions for text analysis, translation, language generation, and other tasks.



### Multi-modal models

The success of transformers as a way to build language models has led AI researchers to consider whether the same approach would be effective for image data. The result is the development of _multi-modal_ models, in which the model is trained using a large volume of captioned images, with no fixed _labels_. An image encoder extracts features from images based on pixel values and combines them with text embeddings created by a language encoder.
![[Pasted image 20240414191149.png]]


- _Image classification_: Identifying to which category an image belongs.
- _Object detection_: Locating individual objects within an image.
- _Captioning_: Generating appropriate descriptions of images.
- _Tagging_: Compiling a list of relevant text tags for an image.
![[Pasted image 20240414193912.png]]


# Summary



Computer vision is built on the analysis and manipulation of numeric pixel values in images. Machine learning models are trained using a large volume of images to enable common computer vision scenarios, such as image classification, object detection, automated image tagging, optical character recognition, and others

While you _can_ create your own machine learning models for computer vision, the Azure AI Vision service provides many pretrained capabilities that you can use to analyze images, including generating a descriptive caption, extracting relevant tags, identifying objects, and others.